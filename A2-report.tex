\documentclass[12pt,fleqn]{article}
\usepackage[margin=0.5in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{trees}

\setlength{\mathindent}{0pt}
\allowdisplaybreaks

\title{COMP 4190 Assignment 2 Answer}
\author{Fidelio Ciandy, 7934456}
\date{}

\begin{document}
\maketitle

\section*{Problem 1}
For my first question I have four different functions

\begin{itemize}
  \item \textbf{queens\_puzzle}
  \begin{itemize}
    \item This function initializes the board with empty cells
    \item It initializes a set to store the queens positions
    \item It initializes an array to store the solutions
    \item It calls the backtrack function
  \end{itemize}

  \item \textbf{backtrack}
  \begin{itemize}
    \item This function is where the backtracking happens
    \item For each row, it recursively iterates through each column and checks
          whether placing a queen at that cell is valid
    \item If the queen can be attacked, it continues to the next column
    \item If we successfully reach the last row, we add that board to the solution
  \end{itemize}

  \item \textbf{is\_attacked}
  \begin{itemize}
    \item This function will iterate through the queens set and see if the
    current queen can be attacked by any queen in that set or not
  \end{itemize}
  
  \item \textbf{determine-queen-attack}
  \begin{itemize}
    \item This function will check whether two different queen position can attack each other
  \end{itemize}

\end{itemize}
    
\section*{Problem 2}
    \begin{itemize}
    \item \textbf{words-from-board}
    \begin{itemize}
        \item This function will iterate through each word, and the board
        \item Check whether if the current word matches any word on the board
        \item Will use the backtrack algorithm to check whether the word match or no
    \end{itemize}

    \item \textbf{backtrack}
    \begin{itemize}
        \item This function is where the backtracking happens
        \item Will iterate through each cell neighbor
        \item Will check if the cell has been visited or not
        \item For each neighbor, if the character is still the same as the word,
        \item It will keep going deeper,
        \item else, it will backtrack
    \end{itemize}

    \item \textbf{FindNeighbors}
    \begin{itemize}
       \item Given a specific entry (row, col), this function will calculate the neighbors
       for that specific (row, col)
    \end{itemize}

    \end{itemize}

\section*{Problem 3}
    \begin{itemize}
    \item \textbf{piles}
    \begin{itemize}
        \item Using dynamic programming,
        \item first, it will initialize the calculation when there's only 1 pile left
        \item calculating and saving for each possible pile
        \item Then, it will iterate through each possible pile, e.g., 2,3,4,..n. 
        \item And calculate the outcome of when we take the left pile or the right pile
        \item will choose the best possible outcome (max)
        \item Lastly, if we have calculated each entry, if the last entry is bigger than 0. 
        \item That means that Alice wins the game
    \end{itemize}

    \end{itemize}
\section*{Problem 4}

\begin{enumerate}[label=(\alph*)]
    \item 
        \begin{tikzpicture}[
        level distance=1.5cm,
        level 1/.style={sibling distance=7cm},
        level 2/.style={sibling distance=3cm},
        level 3/.style={sibling distance=1.5cm},
        every node/.style={circle, draw, minimum size=9mm, align=center},
        edge from parent/.style={draw,-}
        ]

        \node {MAX {= 6}}
        child { node {A1 = 4\\\small MIN}
            child { node {B1 = 5\\\small MAX}
            child { node {3} }
            child { node {5} }
            }
            child { node {B2 = 4\\\small MAX}
            child { node {2} }
            child { node {4} }
            }
        }
        child { node {A2 = 6\\\small MIN}
            child { node {B3 = 6\\\small MAX}
            child { node {6} }
            child { node {1} }
            }
            child { node {B4 = 7\\\small MAX}
            child { node {0} }
            child { node {7} }
            }
        };

        \end{tikzpicture}

    \item - Based on my computation, MAX should choose the right side at the root. \\
          - 6 is the payoff MAX can guarantee despite MIN's action \\
          - minimax corresponds to a worst-case guarantee for MAX because just below MAX, there's 
          MIN which forces MAX to choose the best out of the worst (MAX's worst case). Thus, a worst-case guarantee.

    \item Optimization
    \begin{enumerate}[label=(\roman*)]
        \item The reason why performing a min after max gives min more control over the final outcome is because
        if minimization is done after maximization, and the min moves second, thus, will choose the worst possible outcome
        for the last maximizer. By choosing the worst possible outcome for the max, minimizer can force the final value as small as possible.
        Thus, gives the minimizing player more control over the final outcome.

        \item 
        \begin{align*}
            V &= \max_{a} \min_{b} \max_{c \in children(b)} LeafValue(a,b,c) \\
            &= \max_{a} \min_{b} g(a,b)
        \end{align*}
        Lets try to compute the tree using the formula
            \[ V = \max_{A1, A2} (\min_{B1, B2} (g(A1, B1), g(A1, B2)), \min_{B3, B4} (g(A2,B3), g(A2, B4))) \\
            \]
            \[ 
            = max (min (max(3,5), max(2,4)), min(max(6,1), max(0,7)))
            \]
            \[ 
            = 6
            \]
        \item \[\text{compare } \max_{a}\min_{b} g(a,b) \text{ and } \min_{b}\max_{a} g(a,b)\]
        \[\text{Compute} \rightarrow \min_{b}\max_{a} g(a,b)\]
        Since min goes last but the "max" doesn't know what value min is going to choose, 
        there could be 4 different choices that min will choose:
        \begin{enumerate}
            \item B1, B3
            \item B1, B4
            \item B2, B3
            \item B2, B4
        \end{enumerate}
        
        Thus we define:
        \[
        b \in \{(B1,B3),(B1,B4),(B2,B3),(B2,B4)\}, 
        \quad
        a \in \{A1,A2\}
        \]
        
        For each such pair $b=(b_1,b_2)$, we compute:
        \[
        \max_{a} \; (g(A1,b_1),\; g(A2,b_2))
        \]

        \begin{enumerate}
            \item  \textbf{Evaluate:}  $b=(B1,B3)$
            \[
            \max_{a} ( g(A1,B1),\; g(A2,B3)) = max(max(3,5), max(6,1)) = 6
            \]
            
            \item \textbf{Evaluate:}  $b=(B1,B4)$
            \[
            \max_{a} ( g(A1,B1),\; g(A2,B4)) = max(max(3,5), max(0,7)) = 7
            \]

            \item \textbf{Evaluate:}  $b=(B2,B3)$
            \[
            \max_{a} ( g(A1,B2),\; g(A2,B3)) = max(max(2,4), max(6,1)) = 6
            \]

            \item \textbf{Evaluate:}  $b=(B2,B4)$
            \[
            \max_{a} ( g(A1,B2),\; g(A2,B4)) = max(max(2,4), max(0,7)) = 7
            \]
        \end{enumerate}

        After calculating all this, min will choose the minimum value, thus $ Vmin = 6 $

        From part (2), the game value V is also 6. Thus, they are equal

        \item Suppose two quantities above are equal. This is what's called the Minimax Theorem, the one that's being shown in the slides.
              It implies that revealing the optimal strategy doesn't hurt you. Meaning that, neither player has 
              any benefits from changing strategy or moving order. Thus, the game has a stable optimal strategies.
        
    \end{enumerate}
\end{enumerate}

\section*{Problem 5}

\begin{enumerate}[label=(\alph*)]

    \item\[\sum_X \sum_Y P(X=x, Y=y)= 0.10 + 0.20 + 0.10 + 0.15 + 0.25 + 0.20 = 1\]
    \item\[P(X=0) = 0.10 + 0.20 + 0.10 = 0.40\]
    \[P(X=1) = 0.15 + 0.25 + 0.20 = 0.60\]
    \[P(Y=0) = 0.10 + 0.15 = 0.25\]
    \[P(Y=1) = 0.20 + 0.25 = 0.45\]
    \[P(Y=2) = 0.10 + 0.20 = 0.30\]

    \item Conditional probability formula:
    \[P(X \mid Y) = \frac{P(X \cap Y)}{P(Y)}\]
    Bayes' theorem:
    \[P(X \mid Y) = \frac{P(X)\,P(Y \mid X)}{P(Y)}\]
    \[P(X=1 \mid Y=1)= \frac{0.6(0.25/0.6)}{0.45}= 0.56\]
    \[P(Y=2 \mid X=1)= \frac{0.3(0.20/0.3)}{0.6}= 0.33\]
    \[P(X=0 \mid Y=0)= \frac{0.10}{0.25}= 0.40\]

    \item
    \[P(X=0 \mid Y=1) = \frac{0.2}{0.45} = 0.44\]
    \[P(X=1 \mid Y=1) = \frac{0.25}{0.45} = 0.56\]
    \[P(Y=0 \mid X=0) = \frac{0.1}{0.4} = 0.25\]
    \[P(Y=1 \mid X=0) = \frac{0.2}{0.4} = 0.5\]
    \[P(Y=2 \mid X=0) = \frac{0.1}{0.4} = 0.25\]

    \item Conditional probability identity: \\
    if $P(A|B) = P(A) \text{ and } P(B|A) = P(B)$, then events A and B are independent. \\
    Seeing from $P(X=0 \mid Y=1) = \frac{0.2}{0.45} = 0.44$ and $P(X=0) = 0.10 + 0.20 + 0.10 = 0.40$ \\
    $ 0.44 \neq 0.40 $, thus, X and Y are not independent.

    \item 
    1. Joint probability $P(X,Y) \rightarrow$ the probability that both happens at the same time. 
    In other words, the probability that the AI model correctly predicts "object" at a specific sensor.
    For instance, $P(X=1, Y=1)$, meaning that probability of AI model predict "yes" and the object actually present at Y = 1 is 0.2

    2. Marginal probability $P_x(X) \rightarrow$ the probability of X alone, ignoring Y. 
    In other words, the probability that the object present or not present, independent of Y.

    3. Conditional probability $P(X | Y) \rightarrow$ the probability of X given that Y already happened.
    In other words, the probability of the AI model predicts "object present" or not given an object has been detected previously at Y.
    For instance, $P(X=1 | Y=1)$, is the probability that AI predicts an "object present" given object detected by sensor 1.
\end{enumerate}

\section*{Problem 6}
\begin{enumerate}[label=(\alph*)]
    \item Suppose generate 1000 samples
    \begin{enumerate}[label=(\arabic*)]
        \item Expected samples from each pair (X,Y) out of 1000 samples
        \begin{enumerate}[label=(\Roman*)]
            \item $(X=0,Y=0) = 1000 * 0.1 = 100$
            \item $(X=0,Y=1) = 1000 * 0.2 = 200$
            \item $(X=0,Y=2) = 1000 * 0.1 = 100$
            \item $(X=1,Y=0) = 1000 * 0.15 = 150$
            \item $(X=1,Y=1) = 1000 * 0.25 = 250$
            \item $(X=1,Y=2) = 1000 * 0.2 = 200$
        \end{enumerate}
        \item Samples where X = 0. Y = 1
        \begin{enumerate}[label=(\Roman*)]
            \item $X = 0 \rightarrow 1000 * (0.1 + 0.2 + 0.1) = 400$
            \item $Y = 1 \rightarrow 1000 * (0.2 + 0.25) = 450$
        \end{enumerate}
    \end{enumerate}
    
    \item Simulate sampling, 
    \begin{enumerate}[label=(\Roman*)]
        \item by placing 1000 cards in a box. Each card has label (X, Y), where the number of each card match the expected samples from (a)
        \item mix those 1000 cards
        \item draw one card at random from the box. This represents one sample from this box
    \end{enumerate}

    \item The reason why this simulates to conditional probability $P(X | Y)$ is because,
    by getting the value of Y and restrict the sample only by using that same value of Y. 
    Meaning that we randomly selects only from the reduced set of samples using that same value of Y.
    Thus, this relates to the definition of conditional probability, where the probability of getting X 
    from this reduced set represents $P(X|Y)$, which is the probability of X, given that Y has occured.

    \item Suppose that we generate more and more sample from this distribution. Note the formula of standard deviation.
    Low standard deviation means that data is consistent. On the other hand, high standard deviation means data has high variability.
    Since standard deviation is inversely propotional from N, thus, as sample N increases, the standard deviation decreases. 
    Thus, data is more consistent, and should approach the probabilities in the table.
\end{enumerate}

\section*{Problem 7}

\begin{enumerate}[label=(\alph*)]
    \item 
    \begin{align*}
    & \int_{-\infty}^{\infty} C \; exp(\frac{(x-\mu)^2}{2\sigma^2}) dx = 1 \\
    & \text{Let } u = x - \mu \\
    & \frac{du}{dx} = \frac{d}{dx} (x - \mu) \\
    & \frac{du}{dx} = 1 \\
    & du = dx \\
    & \int_{-\infty}^{\infty} C \; exp(-\frac{u^2}{2\sigma^2}) du = 1 \\
    & C \int_{-\infty}^{\infty} \; exp(-\frac{u^2}{2\sigma^2}) du = 1 \\
    & \text{Let a } = \frac{1}{2\sigma^2} \\
    & \text{Solve for } \rightarrow \int_{-\infty}^{\infty} \; exp(-au^2) du \\
    & \text{Using the Gaussian Integral Property, the above calculation will simplify to } \\
    & \int_{-\infty}^{\infty} \; exp(-au^2) du = \sqrt{\frac{\pi}{a}}\\
    & \text{Now, solve for C }\\
    & C \;\sqrt{\frac{\pi}{a}} = 1 \\
    & C \;\sqrt{\frac{\pi}{\frac{1}{2\sigma^2}}} = 1 \\
    & C = \frac{1}{\sigma\sqrt{2\pi}} \\
    \end{align*}
\end{enumerate}

\end{document}